{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Level Ice Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTES for NILS\n",
    "\n",
    "We're pulling our level ice theory from Von Abedyll et al. and Rabenstein et al., which were designed for very small data granularity. While that is true of IS2 as well, there are actually relatively few inter-return distances that are less than 100 meters. Interpolating the IS2 measurements to 100m segments results in an order of magnitude more points. **This means that the calculated mask has a much finer level of detail in parsing what ice is level than the IS2 data can always resolve.**\n",
    "\n",
    "Questions to consider: \n",
    "\n",
    "1. Do we need this level of granularity? If not, can we adapt the level_ice theory (the gradient threshold perhaps) so that we can deal more directly with the ICESat-2 measurements?\n",
    "    - reasoning behind this: when interpolated to 100m, there are many segments that are \"level\" and \"non-level\" between two actual IS2 returns-- how would we decided to assign the IS2 returns in this case? \n",
    "\n",
    "2. Can we accomplish our analysis reasonably by downscaling everything to 100m? \n",
    "    - Would need some measure of the interpolation accuracy, perhaps some validation with another data source? \n",
    "    - If not, can we think about representing level versus non-level ice as relative fractions within each grid cell/buoy bounding polygon?\n",
    "        - we'd need to carry around the lat/lon/data for IS2 and the 100m lat/lon/mask for level ice for a while (sort each into bounding polygons separately?) \n",
    "\n",
    "3. If we want to continue using the theory (VA and Rabenstein), what's the best way to project the 100m interpolated mask back out the the real lat/lon? How should we make the decisions about point-to-point intervals that have level and non-level segments in between them? \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from scipy.interpolate import lagrange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/glade/scratch/mollyw/external_data/ICESat-2/freeboard/2019.01.01/ATL10-01_20190101005132_00550201_005_02.h5'\n",
    "track = 'gt1r'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_track(file, track):\n",
    "    with h5py.File(file, mode='r') as f:\n",
    "        latvar = f['/%s/freeboard_beam_segment/beam_freeboard/latitude' % track]\n",
    "        latitude = latvar[:]\n",
    "    \n",
    "        # access the longitude\n",
    "        lonvar = f['/%s/freeboard_beam_segment/beam_freeboard/longitude' % track]\n",
    "        longitude = lonvar[:]\n",
    "\n",
    "        # access the freeboard values \n",
    "        datavar = f['/%s/freeboard_beam_segment/beam_freeboard/beam_fb_height' % track]\n",
    "        data = datavar[:]\n",
    "\n",
    "        # access the freeboard uncertainty\n",
    "        unc_var = f['/%s/freeboard_beam_segment/beam_freeboard/beam_fb_sigma' % track]\n",
    "        unc = unc_var[:]\n",
    "\n",
    "        # access the open ocean mask\n",
    "        mask_var = f['/%s/freeboard_beam_segment/height_segments/height_segment_ssh_flag' % track]\n",
    "        mask = mask_var[:]\n",
    "\n",
    "        # access the surface type classification\n",
    "        surf_var = f['/%s/freeboard_beam_segment/height_segments/height_segment_type' % track]\n",
    "        surf = surf_var[:]\n",
    "\n",
    "        #handle FillValue\n",
    "        _FillValue = datavar.attrs['_FillValue']\n",
    "        data[data == _FillValue] = np.nan\n",
    "        unc[unc == _FillValue] = np.nan\n",
    "\n",
    "        # collect time information\n",
    "        timevar = f['/%s/freeboard_beam_segment/delta_time' % track]\n",
    "        time = timevar[:]\n",
    "\n",
    "        return data, latitude, longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_distance(x1, x2, y1, y2):\n",
    "\n",
    "    dist = 2*np.arcsin(np.sqrt(np.sin((x1 - y1)/2)**2 +\n",
    "                               np.cos(x1)*np.cos(y1)*np.sin((x2-y2)/2)**2))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon2dist(data, latitude, longitude):\n",
    "        x_interp = []\n",
    "        for i in range(0, len(data)-1):\n",
    "                x1 = latitude[i]\n",
    "                y1 = longitude[i]\n",
    "\n",
    "                x2 = latitude[i+1]\n",
    "                y2 = longitude[i+1]\n",
    "\n",
    "                dist = 1000 * haversine_distance(x1, x2, y1, y2)\n",
    "                x_interp.append(dist)\n",
    "\n",
    "        distances = np.cumsum(x_interp)\n",
    "        distances = np.insert(distances, 0, 0)\n",
    "\n",
    "        return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_100m(data, latitude, longitude, distances):\n",
    "\n",
    "    x_new = np.arange(0, distances[-1], 100)\n",
    "\n",
    "    new_data = np.interp(x_new, distances, data)\n",
    "    new_lats = np.interp(x_new, distances, latitude)\n",
    "    new_lons = np.interp(x_new, distances, longitude)\n",
    "\n",
    "    return new_data, new_lats, new_lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(data):\n",
    "\n",
    "    \n",
    "    # calculate the gradient of the data\n",
    "    gradient = np.gradient(data)\n",
    "\n",
    "    # create mask \n",
    "    mask = []\n",
    "    for x in range(0, len(gradient)):\n",
    "        # if the data was NaN, the mask is NaN\n",
    "        if np.isnan(gradient[x]):\n",
    "            mask.append(np.nan)\n",
    "        # if the gradient is greater than the threshold, assign it 0 (False)\n",
    "        elif abs(gradient[x]) > 0.04:\n",
    "            mask.append(0)\n",
    "        # else, assign the mask 1 (True)\n",
    "        else:\n",
    "            mask.append(1)\n",
    "\n",
    "    return mask \n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_level_ice(data, latitude, longitude):\n",
    "    \n",
    "    distances = latlon2dist(data, latitude, longitude)\n",
    "\n",
    "    new_data, new_lats, new_lons = interp_100m(data, latitude, longitude, distances)\n",
    "\n",
    "    mask = create_mask(new_data)\n",
    "\n",
    "    return mask, new_lats, new_lons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate test case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lat, lon = read_track(file, track)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, nlat, nlon = detect_level_ice(data, lat, lon)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot mask\n",
    "\n",
    "(I will make this an actual map soon!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "cmap = mpl.colors.ListedColormap(['crimson', 'blue'])\n",
    "plt.scatter(nlat, mask, c = mask, cmap = cmap)\n",
    "# plt.colorbar()\n",
    "\n",
    "plt.xlabel('longitude', fontweight='bold', fontsize=12, color = 'grey')\n",
    "plt.ylabel('latitude', fontweight='bold', fontsize=12, color = 'grey')\n",
    "plt.title('detect_level_ice mask', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d30c40dadb06127d4c95e26ed04059cb0cb6f65303f6eff9e3dbd1b51dca03de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
